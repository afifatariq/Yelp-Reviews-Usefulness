{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "GRU.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCp8Znu1WhFm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Reviews Usefulness using Gated Recurrent Unit\n",
        "\n",
        "Afifa Tariq\n",
        "\n",
        "--------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dxw-z_8FWk6-",
        "colab_type": "text"
      },
      "source": [
        "Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYXoaZEqnY8e",
        "colab_type": "code",
        "outputId": "3a7d6531-f2d1-4144-be46-969150c1030a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.6/dist-packages (1.0.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u0iRPanJWpwk",
        "colab_type": "text"
      },
      "source": [
        "Authenticate with your google account"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJhGS4ynnr4H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri0YOSeqWtHa",
        "colab_type": "text"
      },
      "source": [
        "Download yelp_review.csv file from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnKpH-einugm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "link = 'https://drive.google.com/open?id=1H-XN57SmGOgjXb7xNDaaAB8UpGeIRHpS' # The shareable link\n",
        "fluff, id = link.split('=')\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('yelp_review.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_9Nj0WPW1Ss",
        "colab_type": "text"
      },
      "source": [
        "Read csv file in pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMZqsAhNnY8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = pd.read_csv('yelp_review.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RUfxxCdnY8w",
        "colab_type": "code",
        "outputId": "f4475584-06c0-41d1-f1ae-1749efd1828d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "useful = reviews.loc[reviews['useful'] >40]\n",
        "useful.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3921, 9)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9hUy8ultoiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "usefulNew = useful[:50]\n",
        "usefulNew.to_csv('veryUseful.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imitmIyKnY85",
        "colab_type": "code",
        "outputId": "6944115b-5f6d-4218-a937-f2073db89d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "usefulReviews = reviews\n",
        "usefulReviews.loc[usefulReviews.useful >=1, 'useful'] = 1\n",
        "usefulReviews.head()\n",
        "print(usefulReviews.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5261668, 9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSM9sjspW_16",
        "colab_type": "text"
      },
      "source": [
        "Function to clean the reviews from punctuations, stopwords, etc.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01iY0nMVnY9B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    text = text.lower()    \n",
        "    if True:\n",
        "        text = text.split()\n",
        "        new_text = []\n",
        "        for word in text:\n",
        "            new_text.append(word)\n",
        "        text = \" \".join(new_text)\n",
        "    # Format words and remove unwanted characters\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'\\<a href', ' ', text)\n",
        "    text = re.sub(r'&amp;', '', text) \n",
        "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
        "    text = re.sub(r'<br />', ' ', text)\n",
        "    text = re.sub(r'\\'', ' ', text)\n",
        "    text = text.split()\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops]\n",
        "    text = \" \".join(text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTOxps1LXFgl",
        "colab_type": "text"
      },
      "source": [
        "Applying cleantext function to all the reviews.\n",
        "\n",
        "This function takes a long time to run, so when I am just checking if my code is working, I keep the num = 50000. Otherwise, it should be num=5000000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMVGYJFDnY9K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "be1fd4b2-2aa4-4a7f-cac0-ba547ae157b4"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "revs_list = [[]]\n",
        "useful_list = [[]]\n",
        "num = 50000 # Number of review read\n",
        "k = 0 # Count\n",
        "nolang = [[]]\n",
        "for index, row in usefulReviews.iterrows():\n",
        "    if k >= num:\n",
        "        break\n",
        "    review = row['text']\n",
        "    usefulCount = row.useful\n",
        "    try:\n",
        "      if detect(review) == 'en':\n",
        "          revs_list.append(clean_text(review))\n",
        "          useful_list.append(usefulCount)\n",
        "          k += 1\n",
        "          # Notify for every 5000 reviews\n",
        "          if len(revs_list) % 5000 == 0:\n",
        "              print(len(revs_list), k)\n",
        "    except:\n",
        "      print(\"This row throws error:\", row)\n",
        "      print(review)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "5000 4999\n",
            "10000 9999\n",
            "15000 14999\n",
            "20000 19999\n",
            "25000 24999\n",
            "30000 29999\n",
            "35000 34999\n",
            "40000 39999\n",
            "45000 44999\n",
            "50000 49999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIcaAo-KX0_u",
        "colab_type": "text"
      },
      "source": [
        "Check what review looks like after cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzVs-zWUnY9S",
        "colab_type": "code",
        "outputId": "6639713b-82a2-4e32-f4f8-edd096e9b756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "print(revs_list[1])\n",
        "print(len(revs_list), len(useful_list))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "super simple place amazing nonetheless around since 30 still serve thing started bologna salami sandwich mustard staff helpful friendly\n",
            "50001 50001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo2GdI3aX4jk",
        "colab_type": "text"
      },
      "source": [
        "Converting our cleaned reviews to pandas dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCNnBJwZnY9Z",
        "colab_type": "code",
        "outputId": "76b43199-3386-4739-8e38-5633c2e8d7e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "np_revs = np.asarray([revs_list]).T\n",
        "np_useful = np.asarray([useful_list]).T\n",
        "stacked_revs = np.hstack((np_revs, np_useful))\n",
        "categories = ['text', 'useful']\n",
        "df_reviews_processing = pd.DataFrame(stacked_revs, columns=categories)\n",
        "df_reviews_processing = df_reviews_processing.iloc[1:, :]\n",
        "print(df_reviews_processing.shape)\n",
        "print(df_reviews_processing.head())"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 2)\n",
            "                                                text useful\n",
            "1  super simple place amazing nonetheless around ...      0\n",
            "2  small unassuming place changes menu every ofte...      0\n",
            "3  lester located beautiful neighborhood since 19...      0\n",
            "4  love coming yes place always needs floor swept...      0\n",
            "5  chocolate almond croissant amazing light butte...      0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkynbBUeaY-5",
        "colab_type": "text"
      },
      "source": [
        "Count the number of unique words in the vocabulary and assign an integer to them. Sort them from highest frequency to lowest so that the words used more often are given a smaller integer value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q96FRwlKUr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "all_text2 = ' '.join(str(v) for v in df_reviews_processing.text)\n",
        "# create a list of words\n",
        "words = all_text2.split()\n",
        "# Count all the words using Counter Method\n",
        "count_words = Counter(words)\n",
        "\n",
        "total_words = len(words)\n",
        "sorted_words = count_words.most_common(total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53zCJMzBnY91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "riqjHpbfafKr",
        "colab_type": "text"
      },
      "source": [
        "Convert all reviews based on integer assigned to word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCuOUOQ1Mo3q",
        "colab_type": "code",
        "outputId": "74e7511f-c836-42c9-b547-e4ca729c238b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "seq = []\n",
        "for review in df_reviews_processing.text:\n",
        "  try:\n",
        "    r = [vocab_to_int[w] for w in review.split()]\n",
        "    seq.append(r)\n",
        "\n",
        "  except:\n",
        "    print(review)\n",
        "print (seq[0:3])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[120, 486, 2, 54, 2536, 71, 76, 287, 60, 450, 113, 331, 13298, 4332, 164, 1697, 25, 257, 30], [87, 4822, 2, 1633, 41, 90, 445, 244, 398, 732, 167, 287, 697, 38, 215, 726, 204, 3597, 231, 1112, 359, 539, 165, 857, 110, 3192, 70, 829, 39, 12, 3332, 1965, 544, 13, 43, 228, 54, 708, 123, 12, 79, 261, 652, 1485, 66, 2929, 279, 131, 601, 276, 949], [19651, 528, 380, 807, 76, 34178, 1135, 1203, 148, 1574, 1199, 164, 35, 1485, 17, 610, 784, 261, 12, 531, 167, 1203, 148, 135, 138, 10261, 94, 279, 3732, 19651, 19]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X28Ftwyhanyj",
        "colab_type": "text"
      },
      "source": [
        "Check review length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeVk5UTcnY-U",
        "colab_type": "code",
        "outputId": "cea3bce1-bdad-40b5-9805-7f10f6d34d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        }
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "reviews_len = [len(x) for x in seq]\n",
        "pd.Series(reviews_len).hist()\n",
        "plt.show()\n",
        "pd.Series(reviews_len).describe()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    50000.000000\n",
              "mean        59.543340\n",
              "std         53.447486\n",
              "min          2.000000\n",
              "25%         24.000000\n",
              "50%         43.000000\n",
              "75%         76.000000\n",
              "max        565.000000\n",
              "dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LegPLspCnY-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "usefuls = df_reviews_processing.useful.values.astype(int)\n",
        "reviews_int = [ seq[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
        "encoded_labels = [ usefuls[i] for i, l in enumerate(reviews_len) if l> 0 ]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lMFupg0atOS",
        "colab_type": "text"
      },
      "source": [
        "Convert all reviews to same length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieeLXlR3nY-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pad_features(reviews_int, seq_length):\n",
        "    ''' Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
        "    '''\n",
        "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
        "    \n",
        "    for i, review in enumerate(reviews_int):\n",
        "        review_len = len(review)\n",
        "        \n",
        "        if review_len <= seq_length:\n",
        "            zeroes = list(np.zeros(seq_length-review_len))\n",
        "            new = zeroes+review\n",
        "        elif review_len > seq_length:\n",
        "            new = review[0:seq_length]\n",
        "        \n",
        "        features[i,:] = np.array(new)\n",
        "    \n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t22f1kUDnY-o",
        "colab_type": "code",
        "outputId": "f9a171c2-911d-4b93-99d5-3df593fa6c28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "features = pad_features(reviews_int, 100)\n",
        "features.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjIna9nwazAh",
        "colab_type": "text"
      },
      "source": [
        "Divide data into three sets: Training, Test, Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdkbrOJDnY-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split_frac = 0.8\n",
        "\n",
        "train_x = features[0:int(split_frac*len(reviews_len))]\n",
        "train_y = encoded_labels[0:int(split_frac*len(reviews_len))]\n",
        "remaining_x = features[int(split_frac*len(reviews_len)):]\n",
        "remaining_y = encoded_labels[int(split_frac*len(reviews_len)):]\n",
        "valid_x = remaining_x[0:int(len(remaining_x)*0.5)]\n",
        "valid_y = remaining_y[0:int(len(remaining_y)*0.5)]\n",
        "test_x = remaining_x[int(len(remaining_x)*0.5):]\n",
        "test_y = remaining_y[int(len(remaining_y)*0.5):]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67WAyEKjnY-1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(np.asarray(train_x)), torch.from_numpy(np.asarray(train_y)))\n",
        "valid_data = TensorDataset(torch.from_numpy(np.asarray(valid_x)), torch.from_numpy(np.asarray(valid_y)))\n",
        "test_data = TensorDataset(torch.from_numpy(np.asarray(test_x)), torch.from_numpy(np.asarray(test_y)))\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE95FiuenY-6",
        "colab_type": "code",
        "outputId": "8fe27858-11a6-49d5-f0c5-91fb989d427f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print()\n",
        "print('Sample label size: ', sample_y.size()) # batch_size\n",
        "print('Sample label: \\n', sample_y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sample input size:  torch.Size([50, 100])\n",
            "Sample input: \n",
            " tensor([[    0,     0,     0,  ...,  1118,     4,     9],\n",
            "        [    0,     0,     0,  ...,    18,   382,  4045],\n",
            "        [ 2683,   244, 28751,  ...,  6845,   398,    79],\n",
            "        ...,\n",
            "        [    0,     0,     0,  ...,   456,   166,    28],\n",
            "        [    0,     0,     0,  ...,  1037,    63,   466],\n",
            "        [    8,    53,  1393,  ...,   168,     5,   155]])\n",
            "\n",
            "Sample label size:  torch.Size([50])\n",
            "Sample label: \n",
            " tensor([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0,\n",
            "        0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        1, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDuyT5Qla_en",
        "colab_type": "text"
      },
      "source": [
        "Create GRU model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pq0itd-rnY-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class GRU(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
        "        \"\"\"\n",
        "        Initialize the model by setting up the layers.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding and GRU layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = nn.GRU(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        \"\"\"\n",
        "        Perform a forward pass of our model on some input and hidden state.\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        # embeddings and gru_out\n",
        "        embeds = self.embedding(x)\n",
        "        gru_out, hidden = self.gru(embeds, hidden)\n",
        "    \n",
        "        # stack up gru outputs\n",
        "        gru_out = gru_out.contiguous().view(-1, self.hidden_dim)\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        out = self.dropout(gru_out)\n",
        "        out = self.fc(out)\n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "    \n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        weight = next(self.parameters()).data\n",
        "        \n",
        "        if (train_on_gpu):\n",
        "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda()\n",
        "        else:\n",
        "            hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_()\n",
        "        \n",
        "        return hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hNkC5BAnY_D",
        "colab_type": "code",
        "outputId": "1aac96c5-43c6-4b75-a75c-eaa462ac8824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "vocab_size = len(count_words)+1 # +1 for the 0 padding\n",
        "output_size = 1\n",
        "embedding_dim = 400\n",
        "hidden_dim = 256\n",
        "n_layers = 2\n",
        "net = GRU(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "print(net)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GRU(\n",
            "  (embedding): Embedding(58755, 400)\n",
            "  (gru): GRU(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXzOVOgnnY_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_on_gpu = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9J8I7mybMYS",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgvJ5McHnY_P",
        "colab_type": "code",
        "outputId": "a446854e-82a8-4691-827a-505cd322d0b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        }
      },
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "losses = []\n",
        "validation_losses = []\n",
        "\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
        "\n",
        "\n",
        "counter = 0\n",
        "print_every = 100\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "# move model to GPU, if available\n",
        "if(train_on_gpu):\n",
        "  net = net.cuda()\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "for e in range(epochs):\n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "\n",
        "    # batch loop\n",
        "    for inputs, labels in train_loader:\n",
        "        counter += 1\n",
        "\n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        #h = tuple([each.data for each in h])\n",
        "        h = h.data\n",
        "\n",
        "        # zero accumulated gradients\n",
        "        net.zero_grad()\n",
        "\n",
        "        # get the output from the model\n",
        "        inputs = inputs.type(torch.LongTensor)\n",
        "        if(train_on_gpu):\n",
        "          inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        output, h = net(inputs, h)\n",
        "\n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        \n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            val_h = net.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            net.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "\n",
        "                # Creating new variables for the hidden state, otherwise\n",
        "                # we'd backprop through the entire training history\n",
        "                #val_h = tuple([each.data for each in val_h])\n",
        "                val_h = val_h.data\n",
        "                \n",
        "                inputs = inputs.type(torch.LongTensor)\n",
        "                if(train_on_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "\n",
        "                \n",
        "                output, val_h = net(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            net.train()\n",
        "            losses.append(loss.item())\n",
        "            validation_losses.append(np.mean(val_losses))\n",
        "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
        "                  \"Step: {}...\".format(counter),\n",
        "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1/4... Step: 100... Loss: 0.232735... Val Loss: 0.233733\n",
            "Epoch: 1/4... Step: 200... Loss: 0.203573... Val Loss: 0.230871\n",
            "Epoch: 1/4... Step: 300... Loss: 0.265269... Val Loss: 0.230516\n",
            "Epoch: 1/4... Step: 400... Loss: 0.178806... Val Loss: 0.233915\n",
            "Epoch: 1/4... Step: 500... Loss: 0.190112... Val Loss: 0.237928\n",
            "Epoch: 1/4... Step: 600... Loss: 0.199465... Val Loss: 0.231032\n",
            "Epoch: 1/4... Step: 700... Loss: 0.197887... Val Loss: 0.229321\n",
            "Epoch: 1/4... Step: 800... Loss: 0.244423... Val Loss: 0.229442\n",
            "Epoch: 2/4... Step: 900... Loss: 0.194067... Val Loss: 0.232102\n",
            "Epoch: 2/4... Step: 1000... Loss: 0.216917... Val Loss: 0.236124\n",
            "Epoch: 2/4... Step: 1100... Loss: 0.210297... Val Loss: 0.235378\n",
            "Epoch: 2/4... Step: 1200... Loss: 0.216260... Val Loss: 0.232461\n",
            "Epoch: 2/4... Step: 1300... Loss: 0.218003... Val Loss: 0.236638\n",
            "Epoch: 2/4... Step: 1400... Loss: 0.247009... Val Loss: 0.233564\n",
            "Epoch: 2/4... Step: 1500... Loss: 0.217630... Val Loss: 0.233221\n",
            "Epoch: 2/4... Step: 1600... Loss: 0.235879... Val Loss: 0.230491\n",
            "Epoch: 3/4... Step: 1700... Loss: 0.197897... Val Loss: 0.243199\n",
            "Epoch: 3/4... Step: 1800... Loss: 0.250917... Val Loss: 0.269611\n",
            "Epoch: 3/4... Step: 1900... Loss: 0.200864... Val Loss: 0.255285\n",
            "Epoch: 3/4... Step: 2000... Loss: 0.263859... Val Loss: 0.254712\n",
            "Epoch: 3/4... Step: 2100... Loss: 0.194540... Val Loss: 0.255091\n",
            "Epoch: 3/4... Step: 2200... Loss: 0.207527... Val Loss: 0.246558\n",
            "Epoch: 3/4... Step: 2300... Loss: 0.220134... Val Loss: 0.245160\n",
            "Epoch: 3/4... Step: 2400... Loss: 0.170281... Val Loss: 0.253475\n",
            "Epoch: 4/4... Step: 2500... Loss: 0.115887... Val Loss: 0.288682\n",
            "Epoch: 4/4... Step: 2600... Loss: 0.143979... Val Loss: 0.283989\n",
            "Epoch: 4/4... Step: 2700... Loss: 0.089958... Val Loss: 0.288981\n",
            "Epoch: 4/4... Step: 2800... Loss: 0.189939... Val Loss: 0.282560\n",
            "Epoch: 4/4... Step: 2900... Loss: 0.158157... Val Loss: 0.277223\n",
            "Epoch: 4/4... Step: 3000... Loss: 0.124999... Val Loss: 0.282688\n",
            "Epoch: 4/4... Step: 3100... Loss: 0.146541... Val Loss: 0.284105\n",
            "Epoch: 4/4... Step: 3200... Loss: 0.152012... Val Loss: 0.292869\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGJG3Z2wbWJm",
        "colab_type": "text"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGRX52Tz2SGX",
        "colab_type": "code",
        "outputId": "3f404810-4ad2-4174-bd8f-7a63a7a06b1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Get test data loss and accuracy\n",
        "\n",
        "test_losses = [] # track loss\n",
        "num_correct = 0\n",
        "\n",
        "# init hidden state\n",
        "h = net.init_hidden(batch_size)\n",
        "\n",
        "net.eval()\n",
        "# iterate over test data\n",
        "for inputs, labels in test_loader:\n",
        "\n",
        "    # Creating new variables for the hidden state, otherwise\n",
        "    # we'd backprop through the entire training history\n",
        "    h=h.data\n",
        "    \n",
        "    # get predicted outputs\n",
        "    inputs = inputs.type(torch.LongTensor)\n",
        "    if(train_on_gpu):\n",
        "        inputs, labels = inputs.cuda(), labels.cuda()\n",
        "    \n",
        "    output, h = net(inputs, h)\n",
        "    \n",
        "    # calculate loss\n",
        "    test_loss = criterion(output.squeeze(), labels.float())\n",
        "    test_losses.append(test_loss.item())\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
        "    \n",
        "    # compare predictions to true label\n",
        "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "    num_correct += np.sum(correct)\n",
        "\n",
        "\n",
        "# -- stats! -- ##\n",
        "# avg test loss\n",
        "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
        "\n",
        "# accuracy over all test data\n",
        "test_acc = num_correct/len(test_loader.dataset)\n",
        "print(\"Test accuracy: {:.3f}\".format(test_acc))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.277\n",
            "Test accuracy: 0.606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzmPjH3ObwKA",
        "colab_type": "text"
      },
      "source": [
        "Preprocessing for testing review"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58uyPxqx3k92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_review(test_review):\n",
        "   \n",
        "    pred_text = clean_text(test_review)\n",
        "    #pred_seq = convert_to_ints(pred_text, pred=True)\n",
        "    test_words = pred_text.split()\n",
        "    \n",
        "    # tokens\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in test_words])\n",
        "\n",
        "    return test_ints\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHatQu4t5vwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(net, test_review, sequence_length=100):\n",
        "    \n",
        "    net.eval()\n",
        "    \n",
        "    # tokenize review\n",
        "    test_ints = tokenize_review(test_review)\n",
        "    \n",
        "    # pad tokenized sequence\n",
        "    seq_length=sequence_length\n",
        "    features = pad_features(test_ints, seq_length)\n",
        "    \n",
        "    # convert to tensor to pass into your model\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    \n",
        "    batch_size = feature_tensor.size(0)\n",
        "    \n",
        "    # initialize hidden state\n",
        "    h = net.init_hidden(batch_size)\n",
        "    \n",
        "    if(train_on_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    \n",
        "    # get the output from the model\n",
        "    output, h = net(feature_tensor, h)\n",
        "    \n",
        "    # convert output probabilities to predicted class (0 or 1)\n",
        "    pred = torch.round(output.squeeze()) \n",
        "    # printing output value, before rounding\n",
        "    print('Prediction value, pre-rounding: {:.6f}'.format(output.item()))\n",
        "    \n",
        "    # print custom response\n",
        "    if(pred.item()==1):\n",
        "        print(\"Useful review detected!\")\n",
        "    else:\n",
        "        print(\"Not useful review detected.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADB5xJJJ6AK0",
        "colab_type": "code",
        "outputId": "3a42f135-9cd6-4dea-e64d-b2c1bf6c1987",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "# test code and generate tokenized review\n",
        "test_review = input(\"Please enter a review in English: \\n\")\n",
        "seq_length=100 \n",
        "predict(net, test_review, seq_length)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please enter a review in English: \n",
            "Not bad!! Love that there is a gluten-free, vegan version of the cheese curds and gravy!! Haven't done the poutine taste test yet with smoke's but Im excited to see which is better. However poutini's might win as they are vegan and gluten-free\n",
            "Prediction value, pre-rounding: 0.871396\n",
            "Useful review detected!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}